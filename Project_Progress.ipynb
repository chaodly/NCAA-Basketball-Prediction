{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data, Data Wrangling, Train Dataset and Test Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, zero_one_loss, precision_recall_fscore_support, roc_curve, auc\n",
    "\n",
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only those files with DETAILED RESULTS\n",
    "RegularSeason_df = pd.read_csv('Data/RegularSeasonDetailedResults.csv')\n",
    "Tourney_df = pd.read_csv('Data/TourneyDetailedResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns, season index, day number, win location, numbers of overtime\n",
    "RegularSeason_df = RegularSeason_df.drop(['Season', 'Daynum', 'Wloc', 'Numot'], axis = 1)\n",
    "Tourney_df = Tourney_df.drop(['Season', 'Daynum', 'Wloc', 'Numot'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "WinLosePair_df = RegularSeason_df\n",
    "# WinLosePair_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename\n",
    "TrainDataOriginal_1_df = pd.DataFrame(WinLosePair_df[\\\n",
    "                                           ['Wfgm', 'Wfga', 'Wfgm3', 'Wfga3', 'Wftm', 'Wfta', 'Wor', 'Wdr', 'Wast', 'Wto', 'Wstl', 'Wblk', 'Wpf',\\\n",
    "                                           'Lfgm', 'Lfga', 'Lfgm3', 'Lfga3', 'Lftm', 'Lfta', 'Lor', 'Ldr', 'Last', 'Lto', 'Lstl', 'Lblk', 'Lpf']])\\\n",
    ".rename(columns = {'Wfgm':'fgm_x', 'Wfga':'fga_x', 'Wfgm3':'fgm3_x', 'Wfga3':'fga3_x', 'Wftm':'ftm_x', 'Wfta':'fta_x', 'Wor':'or_x', 'Wdr':'dr_x', 'Wast':'ast_x', 'Wto':'to_x', 'Wstl':'stl_x', 'Wblk':'blk_x', 'Wpf':'pf_x',\\\n",
    "                  'Lfgm':'fgm_y', 'Lfga':'fga_y', 'Lfgm3':'fgm3_y', 'Lfga3':'fga3_y', 'Lftm':'ftm_y', 'Lfta':'fta_y', 'Lor':'or_y', 'Ldr':'dr_y', 'Last':'ast_y', 'Lto':'to_y', 'Lstl':'stl_y', 'Lblk':'blk_y', 'Lpf':'pf_y'})\\\n",
    ".reset_index(drop = True)\n",
    "\n",
    "TrainDataOriginal_2_df = pd.DataFrame(WinLosePair_df[\\\n",
    "                                           ['Lfgm', 'Lfga', 'Lfgm3', 'Lfga3', 'Lftm', 'Lfta', 'Lor', 'Ldr', 'Last', 'Lto', 'Lstl', 'Lblk', 'Lpf',\\\n",
    "                                           'Wfgm', 'Wfga', 'Wfgm3', 'Wfga3', 'Wftm', 'Wfta', 'Wor', 'Wdr', 'Wast', 'Wto', 'Wstl', 'Wblk', 'Wpf']])\\\n",
    ".rename(columns = {'Lfgm':'fgm_x', 'Lfga':'fga_x', 'Lfgm3':'fgm3_x', 'Lfga3':'fga3_x', 'Lftm':'ftm_x', 'Lfta':'fta_x', 'Lor':'or_x', 'Ldr':'dr_x', 'Last':'ast_x', 'Lto':'to_x', 'Lstl':'stl_x', 'Lblk':'blk_x', 'Lpf':'pf_x',\\\n",
    "                  'Wfgm':'fgm_y', 'Wfga':'fga_y', 'Wfgm3':'fgm3_y', 'Wfga3':'fga3_y', 'Wftm':'ftm_y', 'Wfta':'fta_y', 'Wor':'or_y', 'Wdr':'dr_y', 'Wast':'ast_y', 'Wto':'to_y', 'Wstl':'stl_y', 'Wblk':'blk_y', 'Wpf':'pf_y'})\\\n",
    ".reset_index(drop = True)\n",
    "\n",
    "\n",
    "TrainData_df = TrainDataOriginal_1_df.append(TrainDataOriginal_2_df).reset_index(drop = True)\n",
    "TrainData = TrainData_df.values\n",
    "TrainLabel = np.ones((len(WinLosePair_df)*2))\n",
    "TrainLabel[len(WinLosePair_df):] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nizhe\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Test Data\n",
    "# Select those rows whose 'School' index has substring 'NCAA'\n",
    "# Select and then rstrip\n",
    "\n",
    "Season2018_df = pd.read_csv('Data/2018.csv', skiprows = 1)\n",
    "NCAA_df = Season2018_df[Season2018_df['School'].str.contains('NCAA')]\n",
    "NCAA_df['School'] = NCAA_df['School'].map(lambda x: x.rstrip(' NCAA'))\n",
    "# NCAA_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotated manually\n",
    "ncaa2018_df = pd.read_csv('Data/ncaa2018.csv')\n",
    "# ncaa2018_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the all the tournaments together using best performing Adaboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nizhe\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\nizhe\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3027: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "Season2018_stat_df = NCAA_df[['G', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'ORB', 'TRB', 'AST', \n",
    "                              'TOV', 'STL', 'BLK', 'PF']]\n",
    "Season2018_stat_df['TRB'] = Season2018_stat_df['TRB'] - Season2018_df['ORB']\n",
    "Season2018_stat_df.rename(columns = {'TRB': 'DRB'}, inplace = True)\n",
    "Season2018_avg_df = Season2018_stat_df.div(Season2018_stat_df.G, axis = 0).join(NCAA_df['School'])\n",
    "Season2018_avg_df.drop('G', axis=1, inplace=True)\n",
    "Season2018_avg_df.columns = ['fgm', 'fga', 'fgm3', 'fga3', 'ftm', 'fta', 'or', 'dr', 'ast', 'to', \n",
    "                            'stl', 'blk', 'pf', 'School']\n",
    "# Season2018_avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tourney_test_df = Tourney_df[['Wfgm', 'Wfga', 'Wfgm3', 'Wfga3', 'Wftm', 'Wfta', 'Wor', 'Wdr', 'Wast', 'Wto', 'Wstl', 'Wblk', 'Wpf',\\\n",
    "#                                            'Lfgm', 'Lfga', 'Lfgm3', 'Lfga3', 'Lftm', 'Lfta', 'Lor', 'Ldr', 'Last', 'Lto', 'Lstl', 'Lblk', 'Lpf']]\\\n",
    "# .rename(columns = {'Wfgm':'fgm_x', 'Wfga':'fga_x', 'Wfgm3':'fgm3_x', 'Wfga3':'fga3_x', 'Wftm':'ftm_x', 'Wfta':'fta_x', 'Wor':'or_x', 'Wdr':'dr_x', 'Wast':'ast_x', 'Wto':'to_x', 'Wstl':'stl_x', 'Wblk':'blk_x', 'Wpf':'pf_x',\\\n",
    "#                   'Lfgm':'fgm_y', 'Lfga':'fga_y', 'Lfgm3':'fgm3_y', 'Lfga3':'fga3_y', 'Lftm':'ftm_y', 'Lfta':'fta_y', 'Lor':'or_y', 'Ldr':'dr_y', 'Last':'ast_y', 'Lto':'to_y', 'Lstl':'stl_y', 'Lblk':'blk_y', 'Lpf':'pf_y'})\\\n",
    "# .reset_index(drop = True)\n",
    "# TestLabel = np.ones((len(Tourney_test_df)))\n",
    "# Tourney_test_df.head(n=30)\n",
    "\n",
    "# half with first team winning, half with second team winning, to make the class more balanced\n",
    "Tourney_df1 = Tourney_df.iloc[0 : int(len(Tourney_df)/2)]\n",
    "Tourney_df2 = Tourney_df.iloc[int(len(Tourney_df)/2) : ]\n",
    "TestDataOriginal_1_df = pd.DataFrame(Tourney_df1[\\\n",
    "                                           ['Wfgm', 'Wfga', 'Wfgm3', 'Wfga3', 'Wftm', 'Wfta', 'Wor', 'Wdr', 'Wast', 'Wto', 'Wstl', 'Wblk', 'Wpf',\\\n",
    "                                           'Lfgm', 'Lfga', 'Lfgm3', 'Lfga3', 'Lftm', 'Lfta', 'Lor', 'Ldr', 'Last', 'Lto', 'Lstl', 'Lblk', 'Lpf']])\\\n",
    ".rename(columns = {'Wfgm':'fgm_x', 'Wfga':'fga_x', 'Wfgm3':'fgm3_x', 'Wfga3':'fga3_x', 'Wftm':'ftm_x', 'Wfta':'fta_x', 'Wor':'or_x', 'Wdr':'dr_x', 'Wast':'ast_x', 'Wto':'to_x', 'Wstl':'stl_x', 'Wblk':'blk_x', 'Wpf':'pf_x',\\\n",
    "                  'Lfgm':'fgm_y', 'Lfga':'fga_y', 'Lfgm3':'fgm3_y', 'Lfga3':'fga3_y', 'Lftm':'ftm_y', 'Lfta':'fta_y', 'Lor':'or_y', 'Ldr':'dr_y', 'Last':'ast_y', 'Lto':'to_y', 'Lstl':'stl_y', 'Lblk':'blk_y', 'Lpf':'pf_y'})\\\n",
    ".reset_index(drop = True)\n",
    "\n",
    "TestDataOriginal_2_df = pd.DataFrame(Tourney_df2[\\\n",
    "                                           ['Lfgm', 'Lfga', 'Lfgm3', 'Lfga3', 'Lftm', 'Lfta', 'Lor', 'Ldr', 'Last', 'Lto', 'Lstl', 'Lblk', 'Lpf',\\\n",
    "                                           'Wfgm', 'Wfga', 'Wfgm3', 'Wfga3', 'Wftm', 'Wfta', 'Wor', 'Wdr', 'Wast', 'Wto', 'Wstl', 'Wblk', 'Wpf']])\\\n",
    ".rename(columns = {'Lfgm':'fgm_x', 'Lfga':'fga_x', 'Lfgm3':'fgm3_x', 'Lfga3':'fga3_x', 'Lftm':'ftm_x', 'Lfta':'fta_x', 'Lor':'or_x', 'Ldr':'dr_x', 'Last':'ast_x', 'Lto':'to_x', 'Lstl':'stl_x', 'Lblk':'blk_x', 'Lpf':'pf_x',\\\n",
    "                  'Wfgm':'fgm_y', 'Wfga':'fga_y', 'Wfgm3':'fgm3_y', 'Wfga3':'fga3_y', 'Wftm':'ftm_y', 'Wfta':'fta_y', 'Wor':'or_y', 'Wdr':'dr_y', 'Wast':'ast_y', 'Wto':'to_y', 'Wstl':'stl_y', 'Wblk':'blk_y', 'Wpf':'pf_y'})\\\n",
    ".reset_index(drop = True)\n",
    "\n",
    "TestData_df = TestDataOriginal_1_df.append(TestDataOriginal_2_df).reset_index(drop = True)\n",
    "# TestData_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Test_df = pd.merge(ncaa2018_df, Season2018_avg_df, left_on = 'School_x', right_on = 'School', how = 'inner')\n",
    "Test_df = pd.merge(Test_df, Season2018_avg_df, left_on = 'School_y', right_on = 'School', how = 'inner')\n",
    "Results = Test_df['Results'].values\n",
    "\n",
    "# creating labels for tournament testing data\n",
    "TestLabel1 = np.ones((1, int(TestData_df.shape[0] / 2)))\n",
    "TestLabel2 = np.ones((1, int(TestData_df.shape[0] / 2))) * 2\n",
    "TestLabel = np.append(TestLabel1 , TestLabel2)\n",
    "TestLabel = np.append(Results, TestLabel)\n",
    "\n",
    "Test_df = Test_df.drop(['School_x', 'School_y', 'Rk_x', 'G_x', 'Results', 'Rk_y', 'G_y'], axis = 1)\n",
    "\n",
    "# add the past tournament data\n",
    "Test_df = Test_df.append(TestData_df)\n",
    "\n",
    "Test_df = Test_df.astype(int)\n",
    "TestData = Test_df.values\n",
    "# Test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.9720496894409938\n",
      "Precision-Recall-F1\n",
      "(0.9723090244289273, 0.9720496894409938, 0.9720533141577276, None)\n",
      "----------------------------------\n",
      "LDA\n",
      "Accuracy: 0.9306418219461697\n",
      "Precision-Recall-F1\n",
      "(0.9307043323762246, 0.9306418219461697, 0.9306481416521241, None)\n",
      "----------------------------------\n",
      "Naive Bayes\n",
      "Accuracy: 0.8830227743271222\n",
      "Precision-Recall-F1\n",
      "(0.8830914093147544, 0.8830227743271222, 0.8830334329356722, None)\n",
      "----------------------------------\n",
      "KNN\n",
      "Accuracy: 0.8519668737060041\n",
      "Precision-Recall-F1\n",
      "(0.862167430386709, 0.8519668737060041, 0.8505678222744265, None)\n",
      "----------------------------------\n",
      "SVM\n",
      "Accuracy: 0.8944099378881988\n",
      "Precision-Recall-F1\n",
      "(0.8947131378781453, 0.8944099378881988, 0.8944235177884977, None)\n",
      "----------------------------------\n",
      "Decision Tree\n",
      "Accuracy: 0.8478260869565217\n",
      "Precision-Recall-F1\n",
      "(0.8478823742678682, 0.8478260869565217, 0.8477805422778448, None)\n",
      "----------------------------------\n",
      "Accuracy: 0.9140786749482401\n",
      "Precision-Recall-F1\n",
      "(0.9141940153309739, 0.9140786749482401, 0.9140883452659061, None)\n",
      "----------------------------------\n",
      "Adaboost\n",
      "Accuracy: 0.9420289855072463\n",
      "Precision-Recall-F1\n",
      "(0.9421136069251883, 0.9420289855072463, 0.9420349510098632, None)\n",
      "----------------------------------\n",
      "Gradient Boosting\n",
      "Accuracy: 0.9244306418219461\n",
      "Precision-Recall-F1\n",
      "(0.9247844373796861, 0.9244306418219461, 0.9244401176035358, None)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "# lr = LogisticRegression()\n",
    "# lr.fit(TrainData, TrainLabel)\n",
    "print ('Logistic Regression')\n",
    "print ('Accuracy: ', end = '')\n",
    "print (accuracy_score(TestLabel, lr.predict(TestData)))\n",
    "print ('Precision-Recall-F1')\n",
    "print (precision_recall_fscore_support(TestLabel, lr.predict(TestData), average = \"weighted\"))\n",
    "print ('----------------------------------')\n",
    "\n",
    "# LDA\n",
    "# lda = LinearDiscriminantAnalysis()\n",
    "# lda.fit(TrainData, TrainLabel)\n",
    "print ('LDA')\n",
    "print ('Accuracy: ', end = '')\n",
    "print (accuracy_score(TestLabel, lda.predict(TestData)))\n",
    "print ('Precision-Recall-F1')\n",
    "print (precision_recall_fscore_support(TestLabel, lda.predict(TestData), average = \"weighted\"))\n",
    "print ('----------------------------------')\n",
    "\n",
    "# Naive Bayes\n",
    "# gnb = GaussianNB()\n",
    "# gnb.fit(TrainData, TrainLabel)\n",
    "print ('Naive Bayes')\n",
    "print ('Accuracy: ', end = '')\n",
    "print (accuracy_score(TestLabel, gnb.predict(TestData)))\n",
    "print ('Precision-Recall-F1')\n",
    "print (precision_recall_fscore_support(TestLabel, gnb.predict(TestData), average = \"weighted\"))\n",
    "print ('----------------------------------')\n",
    "\n",
    "# KNN\n",
    "# knn = KNeighborsClassifier(n_neighbors = 2)\n",
    "# knn.fit(TrainData, TrainLabel)\n",
    "print ('KNN')\n",
    "print ('Accuracy: ', end = '')\n",
    "print (accuracy_score(TestLabel, knn.predict(TestData)))\n",
    "print ('Precision-Recall-F1')\n",
    "print (precision_recall_fscore_support(TestLabel,knn.predict(TestData), average = \"weighted\"))\n",
    "print ('----------------------------------')\n",
    "\n",
    "# SVM\n",
    "# svm = SVC()\n",
    "# svm.fit(TrainData, TrainLabel)\n",
    "print ('SVM')\n",
    "print ('Accuracy: ', end = '')\n",
    "print (accuracy_score(TestLabel, svm.predict(TestData)))\n",
    "print ('Precision-Recall-F1')\n",
    "print (precision_recall_fscore_support(TestLabel, svm.predict(TestData), average = \"weighted\"))\n",
    "print ('----------------------------------')\n",
    "\n",
    "# Decision Tree\n",
    "# dt = tree.DecisionTreeClassifier()\n",
    "# dt.fit(TrainData, TrainLabel)\n",
    "print ('Decision Tree')\n",
    "print ('Accuracy: ', end = '')\n",
    "print (accuracy_score(TestLabel, dt.predict(TestData)))\n",
    "print ('Precision-Recall-F1')\n",
    "print (precision_recall_fscore_support(TestLabel, dt.predict(TestData), average = \"weighted\"))\n",
    "print ('----------------------------------')\n",
    "\n",
    "# Random Forest\n",
    "# rf = RandomForestClassifier(n_estimators = 100)\n",
    "# rf.fit(TrainData, TrainLabel)\n",
    "print ('Accuracy: ', end = '')\n",
    "print (accuracy_score(TestLabel, rf.predict(TestData)))\n",
    "print ('Precision-Recall-F1')\n",
    "print (precision_recall_fscore_support(TestLabel, rf.predict(TestData), average = \"weighted\"))\n",
    "print ('----------------------------------')\n",
    "\n",
    "# Adaboost\n",
    "# adb = AdaBoostClassifier(n_estimators=100)\n",
    "# adb.fit(TrainData, TrainLabel)\n",
    "print ('Adaboost')\n",
    "print ('Accuracy: ', end = '')\n",
    "print (accuracy_score(TestLabel, adb.predict(TestData)))\n",
    "print ('Precision-Recall-F1')\n",
    "print (precision_recall_fscore_support(TestLabel, adb.predict(TestData), average = \"weighted\"))\n",
    "print ('----------------------------------')\n",
    "\n",
    "# Gradient Boosting\n",
    "# gb = GradientBoostingClassifier(n_estimators=100)\n",
    "# gb.fit(TrainData, TrainLabel)\n",
    "print ('Gradient Boosting')\n",
    "print ('Accuracy: ', end = '')\n",
    "print (accuracy_score(TestLabel, gb.predict(TestData)))\n",
    "print ('Precision-Recall-F1')\n",
    "print (precision_recall_fscore_support(TestLabel, gb.predict(TestData), average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the classifier performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing four classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_error_mean = np.mean(1-train_scores, axis=1)\n",
    "    train_error_std = np.std(1-train_scores, axis=1)\n",
    "    test_error_mean = np.mean(1-test_scores, axis=1)\n",
    "    test_error_std = np.std(1-test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_error_mean - train_error_std,\n",
    "                     train_error_mean + train_error_std, alpha=0.1, color=\"r\"\n",
    "                     )\n",
    "    # color=\"r\"\n",
    "    plt.fill_between(train_sizes, test_error_mean - test_error_std,\n",
    "                     test_error_mean + test_error_std, alpha=0.1, color=\"g\")\n",
    "    # , color=\"g\"\n",
    "    plt.plot(train_sizes, train_error_mean, 'o-', color=\"r\",\n",
    "             label=\" Training error\")\n",
    "    # color=\"r\"\n",
    "    plt.plot(train_sizes, test_error_mean, 'o-', color=\"g\",\n",
    "             label=\" Cross-validation error\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "#     return plt\n",
    "\n",
    "\n",
    "X_test,y_test = TestData, TestLabel\n",
    "X_train,y_train = TrainData, TrainLabel\n",
    "\n",
    "print(\"start\")\n",
    "\n",
    "title = \"Learning Curves for Random Forest\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "estimator = RandomForestClassifier()\n",
    "plot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 0.35), cv=cv, n_jobs=4)\n",
    "\n",
    "print(\"rf finished\")\n",
    "\n",
    "\n",
    "title = \"Learning Curves for Adaboost Classifier\"\n",
    "estimator = AdaBoostClassifier()\n",
    "plot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 0.35), cv=cv, n_jobs=4)\n",
    "\n",
    "print(\"ab finished\")\n",
    "\n",
    "title = \"Learning Curves for Gradient Boosting Tree\"\n",
    "#cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = GradientBoostingClassifier()\n",
    "plot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 0.35), cv=cv, n_jobs=4)\n",
    "\n",
    "print(\"gbm finished\")\n",
    "\n",
    "title = \"Learning Curves for Logistic Regression\"\n",
    "estimator = LogisticRegression()\n",
    "plot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 0.35), cv=cv, n_jobs=-1)\n",
    "\n",
    "print(\"lr finished\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing adaboost and gradient boosting tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 400\n",
    "learning_rate = 1\n",
    "X_test,y_test = TestData, TestLabel\n",
    "X_train,y_train = TrainData, TrainLabel\n",
    "\n",
    "dt_stump=DecisionTreeClassifier(max_depth=1,min_samples_leaf=1)\n",
    "dt_stump.fit(X_train,y_train)\n",
    "dt_stump_err=1.0-dt_stump.score(X_test,y_test)\n",
    " \n",
    "# dt=DecisionTreeClassifier(max_depth=9,min_samples_leaf=1)\n",
    "# dt.fit(X_train,y_train)\n",
    "# dt_err=1.0-dt.score(X_test,y_test)\n",
    " \n",
    "# ada_discrete=AdaBoostClassifier(base_estimator=dt_stump,learning_rate=learning_rate,n_estimators=n_estimators,algorithm='SAMME')\n",
    "# ada_discrete.fit(X_train,y_train)\n",
    "\n",
    "# Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=n_estimators)\n",
    "rf_clf.fit(X_train,y_train)\n",
    "\n",
    "# Gradient Boosting Tree\n",
    "gbm_clf = GradientBoostingClassifier(n_estimators=n_estimators) # default n_estimator is 100\n",
    "gbm_clf.fit(X_train,y_train)\n",
    " \n",
    "# base_estimator=dt_stump\n",
    "ada_real=AdaBoostClassifier(learning_rate=learning_rate,n_estimators=n_estimators,algorithm='SAMME.R')\n",
    "ada_real.fit(X_train,y_train)\n",
    " \n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111)\n",
    "# ax.plot([1,n_estimators],[dt_stump_err]*2,'k-',label='Decision Stump Error')\n",
    "# ax.plot([1,n_estimators],[dt_err]*2,'k--',label='Decision Tree Error')\n",
    " \n",
    "# ada_discrete_err=np.zeros((n_estimators,))\n",
    "# for i,y_pred in enumerate(ada_discrete.staged_predict(X_test)):\n",
    "#     ada_discrete_err[i]=zero_one_loss(y_pred,y_test)    ######zero_one_loss\n",
    "# ada_discrete_err_train=np.zeros((n_estimators,))\n",
    "# for i,y_pred in enumerate(ada_discrete.staged_predict(X_train)):\n",
    "#     ada_discrete_err_train[i]=zero_one_loss(y_pred,y_train)\n",
    "    \n",
    "ada_real_err=np.zeros((n_estimators,))\n",
    "for i,y_pred in enumerate(ada_real.staged_predict(X_test)):\n",
    "    ada_real_err[i]=zero_one_loss(y_pred,y_test)\n",
    "ada_real_err_train=np.zeros((n_estimators,))\n",
    "for i,y_pred in enumerate(ada_real.staged_predict(X_train)):\n",
    "    ada_real_err_train[i]=zero_one_loss(y_pred,y_train)\n",
    "    \n",
    "gbm_err=np.zeros((n_estimators,))\n",
    "for i,y_pred in enumerate(gbm_clf.staged_predict(X_test)):\n",
    "    gbm_err[i]=zero_one_loss(y_pred,y_test)\n",
    "gbm_err_train=np.zeros((n_estimators,))\n",
    "for i,y_pred in enumerate(gbm_clf.staged_predict(X_train)):\n",
    "    gbm_err_train[i]=zero_one_loss(y_pred,y_train)\n",
    "\n",
    "ax.plot(np.arange(n_estimators)+1,ada_real_err,label='Real AdaBoost Test Error',color='orange')\n",
    "ax.plot(np.arange(n_estimators)+1,ada_real_err_train,label='Real AdaBoost Train Error',color='green')\n",
    "# ax.plot(np.arange(n_estimators)+1,rf_err,label='Random Forest Test Error',color='red')\n",
    "# ax.plot(np.arange(n_estimators)+1,rf_err_train,label='Random Forest Train Error',color='blue')\n",
    "ax.plot(np.arange(n_estimators)+1,gbm_err,label='Gradient Boosting Tree Test Error',color='black')\n",
    "ax.plot(np.arange(n_estimators)+1,gbm_err_train,label='Gradient Boosting Tree Train Error',color='yellow')\n",
    " \n",
    "ax.set_ylim((0.0,0.5))\n",
    "ax.set_xlabel('n_estimators')\n",
    "ax.set_ylabel('error rate')\n",
    " \n",
    "leg=ax.legend(loc='upper right',fancybox=True)\n",
    "leg.get_frame().set_alpha(0.7)\n",
    "b=time.time()\n",
    "print('total running time of this example is :',b-a)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve for logistic and adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(TestLabel, log_clf.predict(TestData), pos_label=2)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('ROC')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(TestLabel, ada_real.predict(TestData), pos_label=2)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('ROC')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision, recall, fscore\n",
    "precision_recall_fscore_support(TestLabel, log_clf.predict(TestData), average = \"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(TestLabel, ada_real.predict(TestData), average = \"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(TestLabel, log_clf.predict(TestData)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(TestLabel, ada_real.predict(TestData)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true = TestLabel\n",
    "y_probas = ada_real.predict_proba(TestData)\n",
    "skplt.metrics.plot_roc_curve(y_true, y_probas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true = TestLabel\n",
    "y_probas = log_clf.predict_proba(TestData)\n",
    "skplt.metrics.plot_roc_curve(y_true, y_probas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on this year's tournament on some single games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random game from this year's tournament (UMBC vs. Virginia)\n",
    "t1 = Season2018_avg_df[Season2018_avg_df['School'] == 'Maryland-Baltimore County']\n",
    "t2 = Season2018_avg_df[Season2018_avg_df['School'] == 'Virginia']\n",
    "columns_use = t1.columns.tolist()\n",
    "#columns_use.remove('G')\n",
    "columns_use.remove('School')\n",
    "t1_test = t1[columns_use].reset_index(drop=True)\n",
    "t2_test = t2[columns_use].reset_index(drop=True)\n",
    "test_game = pd.concat([t1_test,t2_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_real.predict(test_game) # UMBC won, predicted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random game from this year's tournament (Missouri vs. Florida State)\n",
    "t1 = Season2018_avg_df[Season2018_avg_df['School'] == 'Missouri']\n",
    "t2 = Season2018_avg_df[Season2018_avg_df['School'] == 'Florida State']\n",
    "columns_use = t1.columns.tolist()\n",
    "columns_use.remove('School')\n",
    "t1_test = t1[columns_use].reset_index(drop=True)\n",
    "t2_test = t2[columns_use].reset_index(drop=True)\n",
    "test_game = pd.concat([t1_test,t2_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_real.predict(test_game) # FSU won, predicted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random game from this year's tournament (Duke vs. Iona)\n",
    "t1 = Season2018_avg_df[Season2018_avg_df['School'] == 'Duke']\n",
    "t2 = Season2018_avg_df[Season2018_avg_df['School'] == 'Iona']\n",
    "columns_use = t1.columns.tolist()\n",
    "#columns_use.remove('G')\n",
    "columns_use.remove('School')\n",
    "t1_test = t1[columns_use].reset_index(drop=True)\n",
    "t2_test = t2[columns_use].reset_index(drop=True)\n",
    "test_game = pd.concat([t1_test,t2_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_real.predict(test_game) # Duke won, predicted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random game from this year's tournament (Arizona vs. Buffalo)\n",
    "t1 = Season2018_avg_df[Season2018_avg_df['School'] == 'Arizona']\n",
    "t2 = Season2018_avg_df[Season2018_avg_df['School'] == 'Buffalo']\n",
    "columns_use = t1.columns.tolist()\n",
    "#columns_use.remove('G')\n",
    "columns_use.remove('School')\n",
    "t1_test = t1[columns_use].reset_index(drop=True)\n",
    "t2_test = t2[columns_use].reset_index(drop=True)\n",
    "test_game = pd.concat([t1_test,t2_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_real.predict(test_game) # Buffalo won, predicted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random game from this year's tournament (Villanova vs. Radford)\n",
    "t1 = Season2018_avg_df[Season2018_avg_df['School'] == 'Radford']\n",
    "t2 = Season2018_avg_df[Season2018_avg_df['School'] == 'Villanova']\n",
    "columns_use = t1.columns.tolist()\n",
    "#columns_use.remove('G')\n",
    "columns_use.remove('School')\n",
    "t1_test = t1[columns_use].reset_index(drop=True)\n",
    "t2_test = t2[columns_use].reset_index(drop=True)\n",
    "test_game = pd.concat([t1_test,t2_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_real.predict(test_game) # Villanova won, predicted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotlist = ['fgm', 'fga', 'fgm3', 'fga3', 'ftm', 'fta', 'or', 'dr', 'ast', 'to', 'stl', 'blk', 'pf']\n",
    "\n",
    "for key in plotlist:\n",
    "    x = WinLosePair_df['W' + key]\n",
    "    y = WinLosePair_df['L' + key]\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(WinLosePair_df['W' + key])), x, 'o-', color = \"r\", label=\" Win\")\n",
    "    plt.plot(range(len(WinLosePair_df['L' + key])), y, '*-', color = \"g\", label=\" Lose\")\n",
    "    plt.xlabel(\"Team\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(key + '_Comparison')\n",
    "    plt.savefig(key + '_Comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = WinLosePair_df['Wfgm'] / WinLosePair_df['Wfga']\n",
    "y = WinLosePair_df['Lfgm'] / WinLosePair_df['Lfga']\n",
    "plt.figure()\n",
    "plt.plot(range(len(WinLosePair_df['Wfgm'])), x, 'o-', color = \"r\", label=\" Win\")\n",
    "plt.plot(range(len(WinLosePair_df['Lfgm'])), y, '*-', color = \"g\", label=\" Lose\")\n",
    "plt.xlabel(\"Team\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('ShootingAverage_Comparison')\n",
    "plt.savefig('ShootingAverage_Comparison')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
